# Available parameters and their default values for the Consul chart.

# global holds values that affect multiple components of the chart.
global:
  enabled: true
  name: null
  domain: consul
  image: "consul:1.6.2"
  imageK8S: "hashicorp/consul-k8s:0.11.0"
  datacenter: dc1
  enablePodSecurityPolicies: false
  gossipEncryption:
    secretName: ""
    secretKey: ""
  bootstrapACLs: false
  tls:
    enabled: false
    serverAdditionalDNSSANs: []
    serverAdditionalIPSANs: []
    verify: false
    httpsOnly: false

# Server, when enabled, configures a server cluster to run. This should
# be disabled if you plan on connecting to a Consul cluster external to
# the Kube cluster.
server:
  enabled: "-"
  image: null
  replicas: 1
  bootstrapExpect: 1

  enterpriseLicense:
    secretName: null
    secretKey: null

  storage: 10Gi
  storageClass: null
  connect: false
  resources: null
  updatePartition: 0

  disruptionBudget:
    enabled: false
    maxUnavailable: null

  extraVolumes: []
  tolerations: ""
  nodeSelector: null
  priorityClassName: ""
  annotations: null
  extraEnvironmentVars: {}

# Client, when enabled, configures Consul clients to run on every node
# within the Kube cluster. The current deployment model follows a traditional
# DC where a single agent is deployed per node.
client:
  enabled: "-"
  image: null
  #join:

  dataDirectoryHostPath: null
  grpc: true
  exposeGossipPorts: false
  resources: null

  extraConfig: |
    {}

  extraVolumes: []
  tolerations: ""
  nodeSelector: null
  affinity: {}
  priorityClassName: ""
  annotations: null
  extraEnvironmentVars: {}
  dnsPolicy: null
  updateStrategy: null

  snapshotAgent:
    enabled: false
    replicas: 1

    configSecret:
      secretName: null
      secretKey: null

# Configuration for DNS configuration within the Kubernetes cluster.
# This creates a service that routes to all agents (client or server)
# for serving DNS requests. This DOES NOT automatically configure kube-dns
# today, so you must still manually configure a `stubDomain` with kube-dns
# for this to have any effect:
dns:
  enabled: "-"
  clusterIP: null
  annotations: null

ui:
  enabled: "-"
  service:
    enabled: true
    type: NodePort
    annotations: null
    additionalSpec: |
                 ports:
                   - name: http
                     port: 80
                     targetPort: 8500
                     nodePort: 30393

# syncCatalog will run the catalog sync process to sync K8S with Consul
# services. This can run bidirectional (default) or unidirectionally (Consul
# to K8S or K8S to Consul only).
#
# This process assumes that a Consul agent is available on the host IP.
# This is done automatically if clients are enabled. If clients are not
# enabled then set the node selection so that it chooses a node with a
# Consul agent.
syncCatalog:
  enabled: false
  image: null
  toConsul: true
  toK8S: true
  k8sPrefix: null
  k8sSourceNamespace: null
  addK8SNamespaceSuffix: true
  consulPrefix: null
  k8sTag: null
  syncClusterIPServices: true
  nodePortSyncType: ExternalFirst
  aclSyncToken:
    secretName: null
    secretKey: null
  nodeSelector: null
  logLevel: info
  consulWriteInterval: null

# ConnectInject will enable the automatic Connect sidecar injector.
connectInject:
  enabled: false
  imageConsul: null
  imageEnvoy: null
  namespaceSelector: null

  certs:
    secretName: null
    caBundle: ""
    certName: tls.crt
    keyName: tls.key

  nodeSelector: null

  aclBindingRuleSelector: "serviceaccount.name!=default"

  overrideAuthMethodName: ""

  centralConfig:
    enabled: true

    defaultProtocol: null

    proxyDefaults: |
      {}

# Mesh Gateways enable Consul Connect to work across Consul datacenters.
meshGateway:
  enabled: false
  globalMode: local
  replicas: 2
  wanAddress:
    port: 443
    useNodeIP: true
    useNodeName: false
    host: ""

  service:
    enabled: false
    type: ClusterIP
    port: 443
    nodePort: null
    annotations: null
    additionalSpec: null

  imageEnvoy: envoyproxy/envoy:v1.10.0
  hostNetwork: false
  dnsPolicy: null
  consulServiceName: ""
  containerPort: 443
  hostPort: null
  enableHealthChecks: true

  resources: |
    requests:
      memory: "128Mi"
      cpu: "250m"
    limits:
      memory: "256Mi"
      cpu: "500m"

  affinity: null
  tolerations: null
  nodeSelector: null
  priorityClassName: ""
  annotations: null

# Control whether a test Pod manifest is generated when running helm template.
# When using helm install, the test Pod is not submitted to the cluster so this
# is only useful when running helm template.
tests:
  enabled: true